services:

  #-----------------------------------
  #---- Private-GPT services ---------
  #-----------------------------------

  private-gpt-ollama:
    image: ${PGPT_IMAGE:-zylonai/private-gpt}:${PGPT_TAG:-0.6.2}-ollama  # x-release-please-version
    user: root
    build:
      context: .
      dockerfile: Dockerfile.ollama
    volumes:
      - ./local_data:/home/worker/app/local_data
    ports:
      - "8001:8001"   # API
    environment:
      PORT: 8001
      PGPT_PROFILES: docker
      PGPT_MODE: ollama
      PGPT_EMBED_MODE: ollama
      PGPT_OLLAMA_API_BASE: http://ollama:11434
      HF_TOKEN: ${HF_TOKEN:-}
    profiles:
      - ""
      - ollama-cpu
      - ollama-cuda
      - ollama-api
    depends_on:
      ollama:
        condition: service_healthy

  private-gpt-llamacpp-cpu:
    image: ${PGPT_IMAGE:-zylonai/private-gpt}:${PGPT_TAG:-0.6.2}-llamacpp-cpu # x-release-please-version
    user: root
    build:
      context: .
      dockerfile: Dockerfile.llamacpp-cpu
    volumes:
      - ./local_data/:/home/worker/app/local_data
      - ./models/:/home/worker/app/models
    entrypoint: sh -c ".venv/bin.python scripts/setup && .venv/bin/python -m private_gpt"
    ports:
      - "8001:8001"
    environment:
      PORT: 8001
      PGPT_PROFILES: local
      HF_TOKEN: ${HF_TOKEN:-}
    profiles:
      - llamacpp-cpu

  #-----------------------------------
  #---- NEW: UI (Gradio) -------------
  #-----------------------------------
  private-gpt-ui:
    image: ${PGPT_IMAGE:-zylonai/private-gpt}:${PGPT_TAG:-0.6.2}-ollama
    # Wähle genau EINE der folgenden Zeilen (lass die andere auskommentiert):
    command: sh -c ".venv/bin/python -m private_gpt.ui"
    # command: sh -c ".venv/bin/python -m private_gpt.ui.app"
    depends_on:
      - private-gpt-ollama
    environment:
      # UI → API (interner Service-Name + Port der API)
      PGPT__API__BASE_URL: http://private-gpt-ollama:8001

      # Gradio hinter Codespaces/Proxy
      GRADIO_SERVER_NAME: 0.0.0.0
      GRADIO_SERVER_PORT: 8000
      GRADIO_ALLOW_REMOTE: "1"
      GRADIO_USE_WEBSOCKETS: "false"   # wichtig in Codespaces
      GRADIO_ROOT_PATH: "/"

      # Optional: falls du die gleichen PGPT-Settings durchreichen willst
      PGPT__UI__HOST: 0.0.0.0
      PGPT__UI__PORT: 8000
    ports:
      - "8000:8000"   # UI
    profiles:
      - ""

  #-----------------------------------
  #---- Ollama services --------------
  #-----------------------------------

  ollama:
    image: traefik:v2.10
    healthcheck:
      test: ["CMD", "sh", "-c", "wget -q --spider http://ollama:11434 || exit 1"]
      interval: 10s
      retries: 3
      start_period: 5s
      timeout: 5s
    ports:
      - "8080:8080"
    command:
      - "--providers.file.filename=/etc/router.yml"
      - "--log.level=ERROR"
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:11434"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./.docker/router.yml:/etc/router.yml:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    profiles:
      - ""
      - ollama-cpu
      - ollama-cuda
      - ollama-api

  ollama-cpu:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    profiles:
      - ""
      - ollama-cpu

  ollama-cuda:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - ollama-cuda
