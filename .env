# Force Qdrant to use a real collection
PGPT__VECTOR_STORE__PROVIDER=qdrant
PGPT__VECTOR_STORE__QDRANT__COLLECTION=privategpt

# (rest of your working config)
PGPT__LLM__MODE=ollama
PGPT__LLM__BASE_URL=http://ollama:11434
PGPT__LLM__MODEL=llama3.1

PGPT__EMBEDDING__MODE=ollama
PGPT__EMBEDDING__MODEL=nomic-embed-text

PGPT__API__HOST=0.0.0.0
PGPT__API__PORT=8001

# give slow/first-call queries time (model load can add ~30â€“90s)
PGPT__OLLAMA__REQUEST_TIMEOUT=600
PGPT__OLLAMA__KEEP_ALIVE=30m

# smaller generations -> finish faster
PGPT__LLM__MAX_NEW_TOKENS=128

# faster ingest (already worked, but helps end-to-end)
PGPT__EMBEDDING__INGEST_MODE=parallel
PGPT__EMBEDDING__COUNT_WORKERS=2
